# =========================================================

# ğŸ”’ Secure Federated Averaging with OpenFHE CKKS (full script)
#   - The original Concrete-ML (TFHE-like) parts are fully replaced.
#   - External class interface of FHEModelAggregator is preserved:
#       * __init__(model_structure, num_clients, ...)
#       * encrypt_model_weights(model_weights_dict)  -> returns per-layer "encrypted" payloads
#       * aggregate_encrypted_models(client_weights_list) -> returns aggregated torch state_dict
#   - Internals now use OpenFHE CKKS to pack/encrypt/add/scale/decrypt.
#   - Minimal changes to the rest of the federated-learning code.
# =========================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import threading
import time
import copy
from math import log2, ceil
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # GUIãªã—ç’°å¢ƒå¯¾å¿œ

# [OPENFHE-CKKS] è¿½åŠ 
from openfhe import *


# =========================
# å°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def _next_pow2(n: int) -> int:
    return 1 << ceil(log2(max(1, n)))


# =========================================================
# ğŸ”’ CKKS-based Model Aggregator (ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã¯ç¶­æŒ)
#   - ä»¥å‰ã¯ Concrete-ML ã®å›è·¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« & æš—å·å®Ÿè¡Œ
#   - ã„ã¾ã¯ OpenFHE CKKS ã§:
#       1) ã‚µãƒ¼ãƒå´: CryptoContext + keys ã‚’ä¸€åº¦ç”¨æ„
#       2) ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: ãƒ¬ã‚¤ãƒ¤ã”ã¨ã« packed plaintext â†’ Encrypt
#       3) ã‚µãƒ¼ãƒ: ãƒ¬ã‚¤ãƒ¤ã”ã¨ã« EvalAddï¼ˆå’Œï¼‰â†’ (1/N) ã®å¹³æ–‡ã‚¹ã‚«ãƒ©ãƒ¼ä¹—ç®— â†’ Decrypt
# =========================================================
class FHEModelAggregator:
    """
    å½¹å‰²:
      - å„ãƒ¬ã‚¤ãƒ¤é‡ã¿ã®å½¢çŠ¶ã‚’è¨˜éŒ²
      - CKKSã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ 1 ã¤æº–å‚™ï¼ˆBatchSize ã¯æœ€å¤§è¦ç´ æ•°ã«åˆã‚ã›ã‚‹ï¼‰
      - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ã‚’ãƒ¬ã‚¤ãƒ¤ã”ã¨ã« flatten â†’ CKKS ã§ pack & encrypt
      - ã‚µãƒ¼ãƒå´ã§æš—å·åŠ ç®— & å®šæ•°ä¹—ç®—ï¼ˆ1/num_clientsï¼‰â†’ å¾©å· â†’ å½¢çŠ¶ã«æˆ»ã—ã¦è¿”ã™
    """

    def __init__(
        self,
        model_structure,
        num_clients=5,
        # ä»¥ä¸‹2ã¤ã¯ Concrete-ML ç‰ˆã®æ®‹ç½®å¼•æ•°ï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ç¶­æŒã®ãŸã‚å—ã‘å–ã‚‹ãŒç„¡è¦–ï¼‰
        scale_factor=100,
        max_value=50,
        # CKKS ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¿…è¦æœ€å°é™ï¼‰
        mult_depth: int = 1,
        scale_mod_size: int = 50,
        security_level: SecurityLevel = SecurityLevel.HEStd_128_classic,
    ):
        self.num_clients = num_clients

        # ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‹ã‚‰é‡ã¿ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ã‚’å–å¾—ã—è¾æ›¸ã«ä¿å­˜
        self.weight_shapes = {}
        max_elems = 0
        for name, param in model_structure.named_parameters():
            self.weight_shapes[name] = param.shape
            max_elems = max(max_elems, int(np.prod(param.shape)))

        # CKKS ã® BatchSize ã¯ 2 ã®å†ªã«ï¼ˆæœ€å¤§è¦ç´ æ•°ã«åŸºã¥ãè¨­å®šï¼‰
        self.batch_size = _next_pow2(max_elems)

        # ---- CKKS æ–‡è„ˆã‚’åˆæœŸåŒ–ï¼ˆConcrete-ML ã®å›è·¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¯å®Œå…¨æ’¤å»ï¼‰ ----
        params = CCParamsCKKSRNS()
        params.SetMultiplicativeDepth(mult_depth)     # å’Œ + å¹³æ–‡ã‚¹ã‚«ãƒ©ãƒ¼ä¹—ç®—ã ã‘ãªã‚‰ 1 ã§ååˆ†
        params.SetScalingModSize(scale_mod_size)      # è¿‘ä¼¼ç²¾åº¦ã®æŒ‡æ¨™ï¼ˆå¿…è¦ã«å¿œã˜ã¦å€‹åˆ¥èª¿æ•´ï¼‰
        params.SetBatchSize(self.batch_size)
        params.SetSecurityLevel(security_level)

        self.cc = GenCryptoContext(params)
        self.cc.Enable(PKESchemeFeature.PKE)
        self.cc.Enable(PKESchemeFeature.KEYSWITCH)
        self.cc.Enable(PKESchemeFeature.LEVELEDSHE)

        # éµç”Ÿæˆï¼ˆä¹—ç®—ã¯å¹³æ–‡å®šæ•°ãªã®ã§å¿…é ˆã§ã¯ãªã„ãŒã€å®‰å…¨å´ã§ç”Ÿæˆï¼‰
        self.keys = self.cc.KeyGen()
        self.cc.EvalMultKeyGen(self.keys.secretKey)

        print(f"ğŸ” CKKS ready: ring dimension = {self.cc.GetRingDimension()}, batch_size = {self.batch_size}")

        # ä»¥å‰ã® Concrete-ML å›è·¯ç½®ãå ´ã¯ãƒ€ãƒŸãƒ¼ã§æ®‹ã™ï¼ˆå¤–éƒ¨å‚ç…§ã•ã‚Œãªã„ãŒäº’æ›ã®ãŸã‚ï¼‰
        self.circuits = {}

    # [REPLACED] Concrete-ML ã®å›è·¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¯å‰Šé™¤ï¼ˆäº’æ›ã®ãŸã‚ç©ºãƒ¡ã‚½ãƒƒãƒ‰ã¯æ®‹ã•ãªã„ï¼‰

    def _flatten_to_len(self, tensor: torch.Tensor, length: int):
        """Tensor â†’ 1D float listï¼ˆå¿…è¦ãªã‚‰ 0 ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰"""
        v = tensor.detach().cpu().float().numpy().reshape(-1).tolist()
        if len(v) > length:
            raise ValueError(f"vector length {len(v)} exceeds batch_size {length}")
        if len(v) < length:
            v = v + [0.0] * (length - len(v))
        return v

    def encrypt_model_weights(self, model_weights_dict):
        """
        å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ã€Œæš—å·åŒ–å…¥åŠ›å½¢å¼ã€ã«æ•´å½¢ã—ã¦è¿”ã™ï¼ˆã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã¯ç¶­æŒï¼‰ã€‚
        Concrete-ML ç‰ˆã¨ã®é•ã„:
          - ä»¥å‰ã¯æ•´æ•°åŒ–ã—ãŸ flatten é…åˆ—ï¼ˆå¹³æ–‡ï¼‰ã‚’è¿”ã—ã¦ã„ãŸ
          - ã„ã¾ã¯ CKKS ã® Ciphertext ã‚’è¿”ã™
        æˆ»ã‚Šå€¤:
          { layer_name: CiphertextCKKSRNS }
        """
        encrypted_weights = {}
        for layer_name, shape in self.weight_shapes.items():
            w_tensor: torch.Tensor = model_weights_dict[layer_name]
            flat = self._flatten_to_len(w_tensor, self.batch_size)  # 0ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Š
            pt = self.cc.MakeCKKSPackedPlaintext(flat)
            ct = self.cc.Encrypt(self.keys.publicKey, pt)
            encrypted_weights[layer_name] = ct
        return encrypted_weights

    def aggregate_encrypted_models(self, client_weights_list):
        """
        å„ãƒ¬ã‚¤ãƒ¤ã”ã¨ã«æš—å·æ–‡ã®ã¾ã¾å¹³å‡ã‚’è¨ˆç®—ã—ã€PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«æˆ»ã™ã€‚
        Concrete-ML ç‰ˆã¨ã®äº’æ›:
          - å¼•æ•° client_weights_list ã¯ã€Œå„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã® state_dictã€
          - å†…éƒ¨ã§ encrypt_model_weights ã‚’å‘¼ã³ã€ãƒ¬ã‚¤ãƒ¤ã”ã¨ã«æš—å·å’Œâ†’å®šæ•°ä¹—ç®—â†’å¾©å·
        æˆ»ã‚Šå€¤:
          aggregated_state_dict (torch state_dict)
        """
        print("\nğŸ”’ Starting CKKS model aggregation...")

        # 1) å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ã‚’ CKKS æš—å·åŒ–
        encrypted_weights_list = []
        for i, client_state_dict in enumerate(client_weights_list):
            print(f"  ğŸ”‘ Encrypting weights from Client {i+1}...")
            enc = self.encrypt_model_weights(client_state_dict)
            encrypted_weights_list.append(enc)

        # 2) ãƒ¬ã‚¤ãƒ¤ã”ã¨ã«æš—å·é›†ç´„
        aggregated_weights = {}
        inv_n = 1.0 / len(encrypted_weights_list)

        for layer_name, shape in self.weight_shapes.items():
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åŒã˜ãƒ¬ã‚¤ãƒ¤ã®æš—å·æ–‡ã‚’é›†ã‚ã‚‹
            layer_cts = [enc[layer_name] for enc in encrypted_weights_list]
            # å’Œ
            c_sum = layer_cts[0]
            for ct in layer_cts[1:]:
                c_sum = self.cc.EvalAdd(c_sum, ct)
            # å¹³å‡ï¼ˆå¹³æ–‡å®šæ•°ã¨ã®ä¹—ç®—ï¼‰
            c_avg = self.cc.EvalMult(c_sum, inv_n)
            # å¾©å·
            pt_avg = self.cc.Decrypt(c_avg, self.keys.secretKey)
            pt_avg.SetLength(self.batch_size)
            # å…ˆé ­ shape è¦ç´ ã¶ã‚“ã®ã¿å–ã‚Šå‡ºã—ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ï¼‰
            num_elems = int(np.prod(shape))
            # openfhe-python ã® Plaintext ã‹ã‚‰å€¤ãƒªã‚¹ãƒˆã‚’å¾—ã‚‹ä¸€èˆ¬å½¢:
            #vals = pt_avg.GetCKKSPackedValue()  # å®Ÿæ•°é…åˆ—ï¼ˆPythonãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã§ã®ä¸€èˆ¬ APIï¼‰
            vals =pt_avg.GetRealPackedValue()
            trimmed = np.array(vals[:num_elems], dtype=np.float32).reshape(shape)
            aggregated_weights[layer_name] = torch.from_numpy(trimmed)

            print(f"  âœ… Aggregated {layer_name} with {len(layer_cts)} clients")

        print("âœ… CKKS model aggregation completed!")
        return aggregated_weights


# =========================================================
# ãƒ¢ãƒ‡ãƒ«ï¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç°¡æ˜“å®Ÿè£…ï¼ˆèª¬æ˜ç›®çš„ï¼šå…ƒã‚³ãƒ¼ãƒ‰ã®ã¾ã¾ï¼‰
# =========================================================

class CustomDataset(Dataset):
    """featuresã¨labelsã‚’å—ã‘å–ã‚‹è–„ã„Dataset"""
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


class NeuralNetwork(nn.Module):
    """
    1å±¤ã®å…¨çµåˆã®ã¿ã‚’æŒã¤æ¥µå°ãƒãƒƒãƒˆï¼ˆèª¬æ˜ãƒ»æ¤œè¨¼ç”¨ï¼‰ã€‚
    åˆæœŸé‡ã¿ã¯å›ºå®šã—ã¦ã€æŒ™å‹•è¿½è·¡ã‚’å®¹æ˜“ã«ã€‚
    """
    def __init__(self, input_size=3, output_size=2):
        super(NeuralNetwork, self).__init__()
        self.fc = nn.Linear(input_size, output_size)
        with torch.no_grad():
            self.fc.weight.data = torch.tensor([[0.1, 0.2, 0.3],
                                                [0.4, 0.5, 0.6]], dtype=torch.float32)
            self.fc.bias.data = torch.tensor([0.1, 0.2], dtype=torch.float32)

    def forward(self, x):
        return self.fc(x)


class LocalTrainer:
    """
    å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã®å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»é‡ã¿å–å¾—ã‚’æ‹…å½“ã™ã‚‹ãƒ˜ãƒ«ãƒ‘
    """
    def __init__(self, model, dataset, optimizer, criterion, device):
        self.model = model
        self.dataset = dataset
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device

    def train(self, epochs):
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        correct = 0

        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_correct = 0
            epoch_samples = 0

            for data, target in self.dataset:
                data, target = data.to(self.device), target.to(self.device)
                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                self.optimizer.step()

                predicted = torch.argmax(output, dim=1)
                epoch_correct += (predicted == target).sum().item()
                epoch_samples += target.size(0)
                epoch_loss += loss.item()

            epoch_accuracy = epoch_correct / epoch_samples * 100
            print(f"    Epoch {epoch + 1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.2f}%")
            total_loss += epoch_loss
            correct += epoch_correct
            total_samples += epoch_samples

        overall_accuracy = correct / total_samples * 100
        print(f"  Local training completed: Overall Accuracy = {overall_accuracy:.2f}%")
        return overall_accuracy

    def evaluate(self, test_dataset=None):
        """ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã§ç²¾åº¦ã¨æå¤±ã‚’è©•ä¾¡"""
        self.model.eval()
        correct = 0
        total = 0
        total_loss = 0.0

        dataset = test_dataset if test_dataset else self.dataset
        with torch.no_grad():
            for data, target in dataset:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                predicted = torch.argmax(output, dim=1)
                correct += (predicted == target).sum().item()
                total += target.size(0)
                total_loss += loss.item()

        accuracy = correct / total * 100
        avg_loss = total_loss / len(dataset)
        print(f"  Evaluation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
        return accuracy, avg_loss

    def get_model_weights(self):
        """ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’ãƒ‡ã‚£ãƒ¼ãƒ—ã‚³ãƒ”ãƒ¼ã§è¿”ã™ï¼ˆç ´å£Šçš„å¤‰æ›´ã‚’é˜²ãï¼‰"""
        return copy.deepcopy(self.model.state_dict())

    def set_model_weights(self, weights):
        """å¤–éƒ¨ã‹ã‚‰ä¸ãˆã‚‰ã‚ŒãŸé‡ã¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šæ›¸ã"""
        self.model.load_state_dict(weights)


def create_simple_data(batch_size, num_samples=100, seed=None):
    """
    ç·šå½¢åˆ†é›¢å¯èƒ½ãªç°¡æ˜“ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆèª¬æ˜ãƒ»æ¤œè¨¼ç”¨ï¼‰
    ãƒ«ãƒ¼ãƒ«: x1 + x2 - x3 > 0 ãªã‚‰ 1ã€ãã‚Œä»¥å¤–ã¯ 0
    """
    if seed is not None:
        np.random.seed(seed)
    features = np.random.randn(num_samples, 3).astype(np.float32)
    labels = ((features[:, 0] + features[:, 1] - features[:, 2]) > 0).astype(np.int64)
    dataset = CustomDataset(features, labels)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)


class Client:
    """
    å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ:
      - æ‰‹å…ƒãƒ‡ãƒ¼ã‚¿ã§ãƒ­ãƒ¼ã‚«ãƒ«å­¦ç¿’
      - å­¦ç¿’å‰å¾Œã®ç²¾åº¦ã‚’å‡ºåŠ›
      - å­¦ç¿’å¾Œã®é‡ã¿ã‚’ã‚µãƒ¼ãƒã¸è¿”ã™
    """
    def __init__(self, client_id, data_seed=None):
        self.client_id = client_id
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = NeuralNetwork(input_size=3, output_size=2).to(self.device)
        self.train_loader = create_simple_data(batch_size=16, num_samples=100, seed=data_seed)
        self.test_loader = create_simple_data(batch_size=16, num_samples=50, seed=data_seed+1000 if data_seed else None)
        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)  # ä½LRã§å®‰å®šåŒ–
        self.criterion = nn.CrossEntropyLoss()
        self.trainer = LocalTrainer(self.model, self.train_loader, self.optimizer, self.criterion, self.device)

    def local_update(self, global_weights, epochs=2):
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã§åˆæœŸåŒ–â†’å­¦ç¿’â†’è©•ä¾¡â†’é‡ã¿è¿”å´"""
        print(f"Client {self.client_id}: Starting local update")
        self.trainer.set_model_weights(global_weights)
        print(f"Client {self.client_id}: Pre-training evaluation:")
        pre_accuracy, _ = self.trainer.evaluate(self.test_loader)
        print(f"Client {self.client_id}: Starting local training...")
        train_accuracy = self.trainer.train(epochs=epochs)
        print(f"Client {self.client_id}: Post-training evaluation:")
        post_accuracy, _ = self.trainer.evaluate(self.test_loader)
        print(f"Client {self.client_id} Summary:")
        print(f"  Pre-training accuracy: {pre_accuracy:.2f}%")
        print(f"  Training accuracy: {train_accuracy:.2f}%")
        print(f"  Post-training accuracy: {post_accuracy:.2f}%")
        print(f"  Accuracy improvement: {post_accuracy - pre_accuracy:.2f}%")
        return self.trainer.get_model_weights()


# =========================================================
# ã‚µãƒ¼ãƒ: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ« & FHEé›†ç´„ã‚’ç®¡ç†ï¼ˆAggregator ã‚’ CKKS ç‰ˆã«ï¼‰
# =========================================================
class FHEServer:
    """
    å½¹å‰²:
      - ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿æŒã¨è©•ä¾¡
      - CKKS é›†ç´„ï¼ˆFHEModelAggregator: CKKS ç‰ˆï¼‰ã‚’å‘¼ã³å‡ºã—
    """
    def __init__(self, num_clients=5):
        self.num_clients = num_clients
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.global_model = NeuralNetwork(input_size=3, output_size=2).to(self.device)

        # [REPLACED WITH OPENFHE-CKKS] ã“ã“ã§ CKKS ç‰ˆã® Aggregator ã‚’ä½¿ç”¨
        self.fhe_aggregator = FHEModelAggregator(
            self.global_model,
            num_clients=num_clients,
            scale_factor=100,   # å¼•æ•°äº’æ›ã®ãŸã‚å—ã‘æ¸¡ã—ã¯ã™ã‚‹ãŒã€CKKS ç‰ˆã§ã¯æœªä½¿ç”¨
            max_value=50        # åŒä¸Š
        )

        self.test_loader = create_simple_data(batch_size=32, num_samples=200, seed=9999)
        self.criterion = nn.CrossEntropyLoss()

    def evaluate_global_model(self):
        """ã‚µãƒ¼ãƒã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡"""
        self.global_model.eval()
        correct = 0
        total = 0
        total_loss = 0.0
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.global_model(data)
                loss = self.criterion(output, target)
                predicted = torch.argmax(output, dim=1)
                correct += (predicted == target).sum().item()
                total += target.size(0)
                total_loss += loss.item()
        accuracy = correct / total * 100
        avg_loss = total_loss / len(self.test_loader)
        print(f"Global Model - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
        return accuracy, avg_loss

    def aggregate_models_with_fhe(self, client_weights_list):
        """
        CKKS ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ã‚’å¹³å‡ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¸é©ç”¨ã€‚
        """
        print("ğŸ”’ FHE-based model aggregation (OpenFHE CKKS)...")
        aggregated_weights = self.fhe_aggregator.aggregate_encrypted_models(client_weights_list)
        self.global_model.load_state_dict(aggregated_weights)
        print("ğŸ”’ FHE Global model updated:")
        for name, param in self.global_model.named_parameters():
            print(f"  {name}: {param.data}")

    def get_global_weights(self):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é…å¸ƒã™ã‚‹ãŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’è¿”ã™"""
        return copy.deepcopy(self.global_model.state_dict())


# =========================================================
# å¹³æ–‡ã‚µãƒ¼ãƒ: æš—å·åŒ–ãªã—ã®é€šå¸¸ã®é€£åˆå­¦ç¿’
# =========================================================
class PlainServer:
    """
    å½¹å‰²:
      - ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿æŒã¨è©•ä¾¡
      - å¹³æ–‡ã§ã®ãƒ¢ãƒ‡ãƒ«é›†ç´„ï¼ˆå˜ç´”å¹³å‡ï¼‰
    """
    def __init__(self, num_clients=5):
        self.num_clients = num_clients
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.global_model = NeuralNetwork(input_size=3, output_size=2).to(self.device)
        self.test_loader = create_simple_data(batch_size=32, num_samples=200, seed=9999)
        self.criterion = nn.CrossEntropyLoss()

    def evaluate_global_model(self):
        """ã‚µãƒ¼ãƒã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡"""
        self.global_model.eval()
        correct = 0
        total = 0
        total_loss = 0.0
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.global_model(data)
                loss = self.criterion(output, target)
                predicted = torch.argmax(output, dim=1)
                correct += (predicted == target).sum().item()
                total += target.size(0)
                total_loss += loss.item()
        accuracy = correct / total * 100
        avg_loss = total_loss / len(self.test_loader)
        print(f"Global Model - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
        return accuracy, avg_loss

    def aggregate_models_plain(self, client_weights_list):
        """
        å¹³æ–‡ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ã‚’å¹³å‡ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¸é©ç”¨ã€‚
        """
        print("ğŸ“Š Plain model aggregation (simple averaging)...")

        # å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ã‚’å¹³å‡
        aggregated_weights = {}
        for layer_name in client_weights_list[0].keys():
            layer_weights = torch.stack([w[layer_name] for w in client_weights_list])
            aggregated_weights[layer_name] = torch.mean(layer_weights, dim=0)

        self.global_model.load_state_dict(aggregated_weights)
        print("ğŸ“Š Plain Global model updated:")
        for name, param in self.global_model.named_parameters():
            print(f"  {name}: {param.data}")

    def get_global_weights(self):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é…å¸ƒã™ã‚‹ãŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’è¿”ã™"""
        return copy.deepcopy(self.global_model.state_dict())


# =========================================================
# æ¯”è¼ƒå®Ÿé¨“é–¢æ•°
# =========================================================
def run_federated_learning_comparison(num_clients=10, num_rounds=10):
    """
    å¹³æ–‡ã¨CKKSæš—å·åŒ–ã®é€£åˆå­¦ç¿’ã‚’å®Ÿè¡Œã—ã€ç²¾åº¦ã¨æ™‚é–“ã‚’æ¯”è¼ƒ
    """
    print("="*80)
    print("ğŸ”¬ FEDERATED LEARNING COMPARISON: Plain vs CKKS Encrypted")
    print("="*80)

    # çµæœæ ¼ç´ç”¨
    results = {
        'plain': {'accuracy': [], 'time': []},
        'ckks': {'accuracy': [], 'time': [], 'key_gen_time': 0}
    }

    # =========================================================
    # 1. å¹³æ–‡ã§ã®é€£åˆå­¦ç¿’
    # =========================================================
    print("\n" + "="*80)
    print("ğŸ“Š PLAIN FEDERATED LEARNING")
    print("="*80)

    plain_server = PlainServer(num_clients=num_clients)
    plain_clients = [Client(client_id=i+1, data_seed=i*100) for i in range(num_clients)]

    print("\nInitial Global Model Evaluation (Plain):")
    initial_accuracy_plain, _ = plain_server.evaluate_global_model()
    results['plain']['accuracy'].append(initial_accuracy_plain)

    for round_num in range(num_rounds):
        print(f"\n{'='*60}")
        print(f"ğŸ“Š PLAIN ROUND {round_num + 1}")
        print(f"{'='*60}")

        round_start = time.time()

        global_weights = plain_server.get_global_weights()
        client_weights_list = []

        for client in plain_clients:
            print(f"\n--- Client {client.client_id} Local Update ---")
            local_weights = client.local_update(global_weights, epochs=2)
            client_weights_list.append(local_weights)

        print(f"\n--- ğŸ“Š Plain Server Aggregation ---")
        plain_server.aggregate_models_plain(client_weights_list)

        round_end = time.time()
        round_time = round_end - round_start

        print(f"\nRound {round_num + 1} Global Model Evaluation (Plain):")
        current_accuracy, _ = plain_server.evaluate_global_model()

        results['plain']['accuracy'].append(current_accuracy)
        results['plain']['time'].append(round_time)

        print(f"ğŸ“Š Plain Round {round_num + 1} completed in {round_time:.2f} seconds!")

    # =========================================================
    # 2. CKKSæš—å·åŒ–ã§ã®é€£åˆå­¦ç¿’
    # =========================================================
    print("\n" + "="*80)
    print("ğŸ”’ CKKS ENCRYPTED FEDERATED LEARNING")
    print("="*80)

    # éµç”Ÿæˆæ™‚é–“ã‚’è¨ˆæ¸¬
    key_gen_start = time.time()
    ckks_server = FHEServer(num_clients=num_clients)
    key_gen_end = time.time()
    key_gen_time = key_gen_end - key_gen_start
    results['ckks']['key_gen_time'] = key_gen_time

    print(f"ğŸ”‘ Key generation time: {key_gen_time:.2f} seconds")

    ckks_clients = [Client(client_id=i+1, data_seed=i*100) for i in range(num_clients)]

    print("\nInitial Global Model Evaluation (CKKS):")
    initial_accuracy_ckks, _ = ckks_server.evaluate_global_model()
    results['ckks']['accuracy'].append(initial_accuracy_ckks)

    for round_num in range(num_rounds):
        print(f"\n{'='*60}")
        print(f"ğŸ”’ CKKS ENCRYPTED ROUND {round_num + 1}")
        print(f"{'='*60}")

        round_start = time.time()

        global_weights = ckks_server.get_global_weights()
        client_weights_list = []

        for client in ckks_clients:
            print(f"\n--- Client {client.client_id} Local Update ---")
            local_weights = client.local_update(global_weights, epochs=2)
            client_weights_list.append(local_weights)

        print(f"\n--- ğŸ”’ CKKS Server Aggregation ---")
        ckks_server.aggregate_models_with_fhe(client_weights_list)

        round_end = time.time()
        round_time = round_end - round_start

        print(f"\nRound {round_num + 1} Global Model Evaluation (CKKS):")
        current_accuracy, _ = ckks_server.evaluate_global_model()

        results['ckks']['accuracy'].append(current_accuracy)
        results['ckks']['time'].append(round_time)

        print(f"ğŸ”’ CKKS Round {round_num + 1} completed in {round_time:.2f} seconds!")

    return results


def plot_comparison_results(results, num_rounds, num_clients):
    """
    æ¯”è¼ƒçµæœã‚’ã‚°ãƒ©ãƒ•åŒ–ã—ã¦ä¿å­˜
    """
    key_gen_time = results['ckks']['key_gen_time']

    # å›³ã®ä½œæˆ
    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # ãƒ©ã‚¦ãƒ³ãƒ‰ç•ªå·ï¼ˆ0ã¯Initialã€1ä»¥é™ã¯å„ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
    rounds = list(range(num_rounds + 1))

    # =========================================================
    # 1. ç²¾åº¦ã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
    # =========================================================
    ax1 = axes[0]
    ax1.plot(rounds, results['plain']['accuracy'], 'o-', label='Plain', linewidth=2, markersize=8)
    ax1.plot(rounds, results['ckks']['accuracy'], 's-', label='CKKS Encrypted', linewidth=2, markersize=8)
    ax1.set_xlabel('Round', fontsize=12)
    ax1.set_ylabel('Accuracy (%)', fontsize=12)
    ax1.set_title('Federated Learning Accuracy Comparison: Plain vs CKKS', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    ax1.set_xticks(rounds)
    ax1.set_xticklabels(['Initial'] + [f'R{i}' for i in range(1, num_rounds + 1)])

    # =========================================================
    # 2. å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
    # =========================================================
    ax2 = axes[1]
    round_nums = list(range(1, num_rounds + 1))

    bar_width = 0.35
    x = np.arange(len(round_nums))

    bars1 = ax2.bar(x - bar_width/2, results['plain']['time'], bar_width,
                    label='Plain', alpha=0.8)
    bars2 = ax2.bar(x + bar_width/2, results['ckks']['time'], bar_width,
                    label='CKKS Encrypted', alpha=0.8)

    ax2.set_xlabel('Round', fontsize=12)
    ax2.set_ylabel('Time (seconds)', fontsize=12)
    ax2.set_title('Federated Learning Execution Time Comparison: Plain vs CKKS', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels([f'Round {i}' for i in round_nums])
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3, axis='y')

    # éµç”Ÿæˆæ™‚é–“ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º
    key_gen_text = f'CKKS Key Generation Time: {key_gen_time:.2f} seconds\n(Not included in round times)'
    ax2.text(0.02, 0.98, key_gen_text, transform=ax2.transAxes,
             fontsize=11, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()

    # ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
    output_file = f'federated_learning_comparison_clients{num_clients}_rounds{num_rounds}.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š Comparison graph saved to: {output_file}")

    plt.close()


def print_comparison_summary(results, num_rounds):
    """
    æ¯”è¼ƒçµæœã®ã‚µãƒãƒªãƒ¼ã‚’æ¨™æº–å‡ºåŠ›
    """
    print("\n" + "="*80)
    print("ğŸ“ˆ COMPARISON SUMMARY")
    print("="*80)

    print(f"\nğŸ”‘ CKKS Key Generation Time: {results['ckks']['key_gen_time']:.2f} seconds")
    print("   (This time is not included in round execution times)")

    print("\n" + "-"*80)
    print("ğŸ“Š Accuracy Comparison (per round):")
    print("-"*80)
    print(f"{'Round':<15} {'Plain (%)':<15} {'CKKS (%)':<15} {'Difference':<15}")
    print("-"*80)

    for i in range(num_rounds + 1):
        round_name = "Initial" if i == 0 else f"Round {i}"
        plain_acc = results['plain']['accuracy'][i]
        ckks_acc = results['ckks']['accuracy'][i]
        diff = ckks_acc - plain_acc
        print(f"{round_name:<15} {plain_acc:>8.2f}%      {ckks_acc:>8.2f}%      {diff:>+8.2f}%")

    print("\n" + "-"*80)
    print("â±ï¸  Execution Time Comparison (per round):")
    print("-"*80)
    print(f"{'Round':<15} {'Plain (s)':<15} {'CKKS (s)':<15} {'Overhead':<15}")
    print("-"*80)

    for i in range(num_rounds):
        plain_time = results['plain']['time'][i]
        ckks_time = results['ckks']['time'][i]
        overhead = ((ckks_time / plain_time) - 1) * 100
        print(f"Round {i+1:<8} {plain_time:>8.2f}s      {ckks_time:>8.2f}s      {overhead:>+8.1f}%")

    # å¹³å‡å€¤
    avg_plain_time = np.mean(results['plain']['time'])
    avg_ckks_time = np.mean(results['ckks']['time'])
    avg_overhead = ((avg_ckks_time / avg_plain_time) - 1) * 100

    print("-"*80)
    print(f"{'Average':<15} {avg_plain_time:>8.2f}s      {avg_ckks_time:>8.2f}s      {avg_overhead:>+8.1f}%")
    print("-"*80)

    print("\n" + "="*80)


# =========================================================
# å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# =========================================================
def main():
    """
    æ¯”è¼ƒå®Ÿé¨“ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    num_clients = 10
    num_rounds = 10

    # æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ
    results = run_federated_learning_comparison(num_clients=num_clients, num_rounds=num_rounds)

    # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
    print_comparison_summary(results, num_rounds)

    # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    plot_comparison_results(results, num_rounds, num_clients)

    print("\nâœ… All comparisons completed!")


if __name__ == "__main__":
    main()

